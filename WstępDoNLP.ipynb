{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "WstępDoNLP.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NonRelDat/twitt/blob/main/Wst%C4%99pDoNLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Wprowadzenie do Uczenia Maszynowego\n",
        "\n",
        "## Wstęp do Przetwarzania Języka Naturalnego\n",
        "\n",
        "Na tym ćwiczeniu omawiane są podstawy NLP na przykładzie kilku prostych narzędzi i metod modelowania.\n",
        "\n",
        "Na początku omówimy podstawowe metody analizy tekstu. Do tego zadania użyjemy biblioteki [spacy](https://spacy.io/). Nie jest ona jedynym narzędziem do tego celu, ale jest uznana za jedną z bardziej wydajnych. Inne przykłady to [NLTK](https://www.nltk.org/) (starsza i uważana za mniej nowoczesną) albo [AllenNLP](https://allenai.org/allennlp).\n",
        "\n",
        "\n",
        "Najpierw zainstalujemy bardziej aktualną wersję spacy niż ta, która jest standardowo dostępna w Google Colab:"
      ],
      "metadata": {
        "id": "AZPA6CkesG3I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U spacy"
      ],
      "metadata": {
        "id": "h4WSSVkTtlsw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Potem zainstalujemy model o nazwie `pl_core_news_lg` poleceniem z dokumentacji. Lista modeli jest dostępna [tutaj](https://spacy.io/usage/models):"
      ],
      "metadata": {
        "id": "iSIpx5uGSIYV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download pl_core_news_lg"
      ],
      "metadata": {
        "id": "09ZEBHRNtCse"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Teraz można zaimportować bibliotekę `spacy` i użyć metodę `load` żeby wczytać wyżej zainstalowany model. Metoda ta zwraca objekt który będziemy używać do analizy tekstu:"
      ],
      "metadata": {
        "id": "kVeSCbmoScJx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "8YL-DInnsImS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tu wkleiłem kawałek transkrypcji z posiedzenia sejmowego ze stycznia 2022:"
      ],
      "metadata": {
        "id": "qjRPtuz_S8vF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_sample='Szanowny Panie Marszałku! Wysoka Izbo! Ustawa zapowiadana jako naprawa, chciałoby się rzec, głęboka korekta Nowego Ładu podatkowego poza rekompensatą strat, jeśli chodzi o zakłady pracy chronionej i zakłady aktywności zawodowej, to w gruncie rzeczy jakaś farsa. Tak naprawdę to jest zalegalizowanie rozporządzenia ministra finansów z 7 stycznia 2022 r., rozporządzenia wydanego z przekroczeniem delegacji ustawowych i naruszeniem konstytucji. Próbujecie uporządkować kawałek, ułameczek problemów, które w tym bublu prawnym, jakim jest Nowy Ład, wygenerowaliście. Po tygodniu obowiązywania ustawy okazało się, że ci, którzy mieli zyskiwać, tracą już dzisiaj, ich wypłaty są niższe, a miały być przecież wyższe. Do tego nikt nie wie, jaki zapłaci podatek. To już norma, jeśli chodzi o wasze działania - mniej znaczy więcej, jak u Orwella. To, co proponujecie, to jest zwykłe mydlenie oczu. W sprawie zaliczek: technicznie rzecz ujmując, płatnik będzie obliczał wysokość zaliczki według zasad z 2022 r., następnie według zasad z 2021 r. Jeśli będzie różnica taka, że zaliczka według zasad z 2022 r. będzie wyższa niż ta z 2021 r., to nie będzie pobierać tej różnicy, a definitywne rozliczenie nastąpi dopiero w zeznaniu rocznym. Taki mechanizm jest sposobem odroczenia płatności wyższej niż w 2021 r. zaliczki na podatek, ale nie jest zwolnieniem podatnika z obowiązku stosowania przepisów z 2022 r. W praktyce dla niektórych podatników może to oznaczać konieczność dopłaty podatku w grudniu 2022 r. albo w rozliczeniu rocznym. Podkreślam, te zmiany nie prowadzą do obniżenia podatków, a jedynie są modyfikacją obowiązków płatników i pracodawców, zleceniodawców i organów rentowych w zakresie poboru zaliczki na podatek dochodowy od osób fizycznych. To jest kolejne obciążenie administracyjne dla płatników, czyli polskich przedsiębiorców i ich księgowych. Projekt jest napisany na kolanie - mówiłem to w ramach pierwszego czytania - i to już jest jakaś tradycja. Czas wreszcie przejść z deklaracji do czynów. Ciągle, tak jak prezydent, mówicie, że się uczycie. A nie sztuka się uczyć, trzeba się nauczyć i wyciągać lekcje i wnioski z dotychczasowych działań. Biuro Legislacyjne po raz kolejny złożyło zastrzeżenie, że nie było w stanie przeanalizować zapisów ustawy, w praktyce - że nie ma pewności co do poprawności jej zapisów. Mało tego, w ustawie są zapisy, które są nie do zaakceptowania i nie do przyjęcia. To jest opinia Biura Legislacyjnego Sejmu. Ale wy oczywiście wszystko możecie przegłosować, dowiedliście tego wczoraj na posiedzeniu komisji. To, że narażacie podatników i płatników na nieprzewidywalne dziś konsekwencje, nie ma dla was w gruncie rzeczy znaczenia. Musi być tak, jak chcecie, i tyle. Wczoraj na posiedzeniu komisji finansów dowiedliście, że nie rozumiecie rzeczywistych skutków Nowego Ładu. Nie dociera do was np. to, że tak naprawdę zlikwidowaliście kwotę wolną. Ona była niska do tej pory, ale była. A dzisiaj kwota wolna wynosi zero, bo dochody do 30 tys. zł są opodatkowane stawką 9%, dochody między 30 a 120 tys. zł - stawką 26%, a dochody ponad 120 tys. zł - stawką 41%. Nie zapominajcie, że podatnicy jeszcze muszą zapłacić ZUS - 19,52%. Jakiś łoś was opętał, takie mam wrażenie. Nie dociera do was, że w Polsce przestało się już, praktycznie rzecz biorąc, opłacać pracować. Do tego te rozwiązania prowadzą. Nie ma rozwiązań, które zapowiadaliście, jeśli chodzi o korektę Nowego Ładu. Nie ma w dalszym ciągu ulgi dla klasy średniej, dla zleceniobiorców, dla świadczeń emerytalnych, rentowych. Nie ma żadnej propozycji co do rozliczania rodziców samotnie wychowujących dzieci, a taką deklarację też słyszeliśmy. Nie ma rekompensaty dla organizacji pozarządowych. Nie ma propozycji, o której mówił też premier: złotówka za złotówkę. I nie odnosicie się w ogóle do konsekwencji Nowego Ładu (Dzwonek) w działalności gospodarczej, a one są tu dramatyczne, dramatyczne. Czas albo dokonać głębokiej korekty, a najlepiej posłuchać tego, co mówiliśmy od samego początku i tę część dotyczącą przedsiębiorców po prostu zawiesić. Dziękuję bardzo. '"
      ],
      "metadata": {
        "id": "duKG2uoEsvII"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Składnia tekstu\n",
        "\n",
        "Teraz można użyć objektu wyżej żeby zanalizować tekst podając go jako arguemnt wywołania objektu zwróconego przez metodę `load` wyżej:"
      ],
      "metadata": {
        "id": "OqStflDQTLZP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "5SenvbNguW-k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "W wyniku otrzymamy objekt reprezentujący cały dokument. Dokumetacja jest dostępna [tutaj](https://spacy.io/api/doc).\n",
        "\n",
        "Zacznijmy od sprawdzenia jak sobie biblioteka poradziła z podziałem tekstu na zdania. Przeiteruj wszystkie elementy `sents` dokumentu i wypisz metodą print:"
      ],
      "metadata": {
        "id": "nkkWN090U4se"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "3HgLZzx3uXja"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Kolejne zadania będą bardziej czytelne jeśli je uruchomimy na pojedynczym zdaniu zamiast całego tekstu. Zamień `sents` na listę i zapisz dowolny element tej listy w osobnej zmiennej `sent`.\n",
        "\n",
        "Potem przeiteruj kolejne elementy tego zdania żeby odczytać poszczególne tokeny. Dokumentacja tokenu jest [tutaj](https://spacy.io/api/token). \n",
        "\n",
        "Dla każdego tokenu wypisz:\n",
        "* `text`\n",
        "* `tag_` - tekstowa postać tagu POS\n",
        "* `pos_` - bardziej ogólny opis POS\n",
        "* `dep_` - opis parsowania zależnościowego\n",
        "* `lemma_` - lemat wyrazu\n",
        "* `morp_` - szczegółowy opis morfologii wyrazu"
      ],
      "metadata": {
        "id": "rQ9Y7RNpVczL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Xyd2u5Vmubuc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Parsowanie zależnościowe opisuje związki między wyrazami w zdaniu. Żeby obejrzeć graf zależności można użyć załączonej biblioteki `displacy`. Zaimportuj moduł `displacy` z biblioteki `spacy` i użyj jego funkcji `render` żeby narysować obraz.\n",
        "\n",
        "Pierwszy arguemnt metody `render` jest objekt jaki chcemy narysować - w naszym przypadku jest to powyższe zdanie. Argument `style` ustawiamy na rodzaj wykresu jaki chcemy narysować - w naszym wypadku ma to być `'dep'`. Dodatkowo musimy ustawić argument `jupyter` na `True` żeby się wykres wyświetlił w środowisku notebookowym:"
      ],
      "metadata": {
        "id": "Vp2kduvNWljU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "gpbMq-TwurKj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## NER\n",
        "\n",
        "Kolejnym ciekawym aspektem analizy jest rozpoznawanie encji - tzw. Named Entity Recognition. Dla całego przeiteruj atrybut `ents` i wypisz odpowiedznio `text` oraz `label_`:"
      ],
      "metadata": {
        "id": "J7Y4ELx5XvRo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "WVU-ZlY2wWvh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Displacy też wspiera podświetlanie NER. Dla powyższego zdania (tego samego którego użyliśmy do wykresu parsowania zależnościowego) narysuj wykres o stylu `'ent'`. Jeśli w twoim zdaniu nie ma żadnych nazw, to wybierz jakieś inne zdanie:"
      ],
      "metadata": {
        "id": "ocnYgZzMYLtc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "H-dRzKQHxawS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## word2vec\n",
        "\n",
        "Kolejną ciekawą cechą to są wektory rzeczywiste opisujące poszczególne wyrazy.\n",
        "\n",
        "Wypis następujące cechy dla poszczególnych tokenów wybranego zdania:\n",
        "* `has_vector`\n",
        "* `is_oov`\n",
        "* `vector_norm`"
      ],
      "metadata": {
        "id": "4yAdHZJPjxUY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "CkJbnTNkyFZb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "No i oczywiście jest sam `vector`, ale jest on trochę długi żeby go wypisywać metodą `print` dla każdego wyrazu. Zamiast tego narysujemy wykres z położeniami wektorów w przestzeni.\n",
        "\n",
        "Najpierw przepisz wszystkie wektory dla całego tekstu w słowniku mapującym tekstu wyrazu na jego wektor:"
      ],
      "metadata": {
        "id": "svWszDcIkMk9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "VR55E2B-CQkh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Niestety nie możemy rysować wszystkie 300 wynmiarów wektora, więc zastosujemy popularną metodę do rzutowania wszystkich wymiarów do 2. Zaimportuj model `TSNE` z modułu `sklearn.manifold` i skonstruuj jego objekt podając w metodzie inicującej argument `n_components` równy 2. Potem zastosuj metodę `fit_transform` nowego objektu na wszystkich wektorach (na `values()` słownika wyżej) i zrób nowy słownik z nowo uzyskanymi wekotrami 2-wymiarowymi:"
      ],
      "metadata": {
        "id": "gli-fRIGkpi4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "5196XVVFCbAO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Żeby zacząć rysować w środowisku notebookowym wystarczy uruchomić polecenie magiczne `%pylab inline`. Zaimportuje to automatycznie numpy, matplotlib i ustawi odpowiednio rysowanie wykresów:"
      ],
      "metadata": {
        "id": "AWXYksFVlzqx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "RQrTXNV6DwIO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Teraz stwórz nowy wykres poleceniem `figure` i ustaw w nim atrybut `figsize` na parę `(15,15)`. W następnej linii uruchom metodę `scatter` i podaj w niej kolejno pierwszą i drugą kolumnę wszystkich wektorów z tablicy wektorów 2D jaką stworzyliśmy wyżej:"
      ],
      "metadata": {
        "id": "wyWNhfMxmMHs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "BK_pdCXnDNbg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Możesz dodatkowo użyć metody `gca()` żeby pobrać objekt `Axes` który udostępnia metodę `text`. Metoda ta bierze 3 argumenty: x, y i string. Możesz użyć tej metody żeby wpisać odpowiednie wyrazy w odpowiednich miejscach na wykresie iterując po kolei słownik wektorów 2D.\n",
        "\n",
        "Ponieważ wykres jest z duży i nie widać wszystkich wyrazów razem, można użyć metod `xlim` i `ylim` żeby ograniczyć zakres tego co się wyświetla na ekranie. Niestety metody te nie mają wpływu na metodę `text`, więc w jej przypadku musisz samemu dokonać filtracji poleceniem `if`:"
      ],
      "metadata": {
        "id": "lZo_0KNPm0Il"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "DiKu2S4TESfX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Praca domowa 1\n",
        "\n",
        "Narysuj wykres TSNE reprezentacji wektorowej jakiegoś dłuższego tekstu - powieści, artykułu zbioru poezji, ..."
      ],
      "metadata": {
        "id": "PbpdPnPEnv3I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dalsze rozważania na temat word2vec\n",
        "\n",
        "Model word2vec opisany wyżej może być użyty do rozwiązywania wielu zadań takich jak klasyfikacja, clustering, czy liczenie podobieństwa. Jest tez ciekawe zastosowanie polegające na wyszukiwaniu (generowaniu?) wyrazów. Do tego celu nie możemy jednak użyć biblioteki spacy. Ściągnijmy najpierw model word2vec wytrenowany na polskich tekstach z tego adresu:\n",
        "\n",
        "https://github.com/sdadas/polish-nlp-resources/releases/download/v1.0/word2vec.zip\n",
        "\n",
        "Ściągnij i rozpakuj powyższy plik:"
      ],
      "metadata": {
        "id": "LpfhJRNN5eUM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "AGkkkxxI2gZ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Zaimportuj klasę `KeyedVectors` z biblioteki `gensim.models` i użyj metody `load` żeby wczytać plik `*.bin` rozpakowany wyżej:"
      ],
      "metadata": {
        "id": "56cMBatfzGKx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "kZnmCbH84BKz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Wczytany model umożliwia wyszukanie wyrazów podobnych do podanego metodą `similar_by_word`. Użyj tej metody do wyszukania wyrazu `tata`:"
      ],
      "metadata": {
        "id": "R6xk-Wn4zXLf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "xijkEdZ74GI2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model word2vec jest często używany do przedstawiania operacji arytmetycznych jakie można wykonywać na tych wektorach. Na przykład, odejmując \"mężczyzna\" od \"tata\" i dodając do wyniku \"kobieta\" powinniśmy otrzymać \"mama\".\n",
        "\n",
        "Do tego można użyć metody `most_similar` podając w argumencie `positive` listę `['kobieta','tata']` a w argumencie `negative` listę `['mężczyzna']`:"
      ],
      "metadata": {
        "id": "RO6IOJA7zpMF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "jaTfMuHC4Y10"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Praca domowa 2\n",
        "\n",
        "Wymyśl więcej przykłądów jak ten powyżej. Niektóre (większość) średnio działają (zależy to od rozmiaru i jakości modelu), ale niektóre mogą być ciekawe."
      ],
      "metadata": {
        "id": "kXFvkt4Q0U7W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modelowanie języka\n",
        "\n",
        "W tym zadaniu wytrenujemy prosty statystyczny model języka i sprawdzimy jego budowę i możliwości zastosowania.\n",
        "\n",
        "Do budowania modelu użyjemy pakietu SRILM. Jak zwykle, nie jest on jedynym narzędziem do tego zastosowania, ale jest jednym z najstarszych i najbardziej cytowanych:"
      ],
      "metadata": {
        "id": "xIit4mro5Z-2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -N http://users.pja.edu.pl/~danijel/zum/srilm-bin.tar.xz\n",
        "!tar xf srilm-bin.tar.xz bin/i686-m64/ngram bin/i686-m64/ngram-count\n",
        "%mv bin/i686-m64/* /usr/local/bin"
      ],
      "metadata": {
        "id": "f3KjnLJMMjrI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Modele języka trenujemy na tzw. korpusach tekstów. Taki korpus to po prostu lista poprawnych zdań w wybranym języku - jedno zdanie na linię. Zróbmy taki miniaturowy przykładowy korpus zawierający 3 zdania:\n",
        "```\n",
        "ala ma kota\n",
        "ala ma psa\n",
        "jan ma kota\n",
        "```\n",
        "\n",
        "Zapisz te zdania do pliku `test.txt`:"
      ],
      "metadata": {
        "id": "wqh1VoQwNIfN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "RZshA-40M3iy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Do utworzenia modelu języka użyjemy programu `ngram-count`. Program ten ma mnóstwo opcji, które można obejrzeć uruchamiając go z opcją `-help`. Nas będą interesować następujące ustawienia:\n",
        "\n",
        " * `-text test.txt` - tym ustawimy plik źródłowy z korpusem\n",
        " * `-order 3` - tym ustawimy to, że chcemy mieć model 3-gramowy\n",
        " * `-wbdiscount` - użyjemy metodę wygładzania Witten-Bell\n",
        " * `-lm out.arpa` - wynik zapiszemy do pliku `out.arpa`"
      ],
      "metadata": {
        "id": "S9Q8ymv1NpQc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ZwQzOQUoNKrE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Wypiszmy zawartość pliku `out.arpa`:"
      ],
      "metadata": {
        "id": "FX7fra7JNuSM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "JFtmOzfONrh9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Format tego pliku jest dosyć prosty i czytelny. Składa się z nagłówka zaczynającego od tokenu `/data/` i zawierającego liczność poszczególnych n-gramów. Potem mamy kolejne sekcje, każda zawierająca listę poszczególnych n-gramów.\n",
        "\n",
        "Każdy n-gram jest opisany dwoma lub trzema polami oddzielonymi znakami `\\t`:\n",
        "* prawdopodobieństwo danego n-gramu w skali logarytmicznej\n",
        "* opis samego n-gramu (tokeny/słowa oddzielone spacją)\n",
        "* opcjonalnie tzw. \"*back-off weight*\" też w skali log\n",
        "\n",
        "Back-off jest metodą do określenia prawdopodobieństwa n-gramów wyższego stopnia użwyając n-gramów niższego. Z tego powodu, najwyższe n-gramy (w naszym przypadku 3-gramy) nie mają policzonych wag back-off. Algorytm liczenia prawdopodonieństwa n-gramu jest następujący:\n",
        "\n",
        "* jeśli na liście jest dokładnie ten n-gram którego szukamy, bierzemy jego prawdopodobieństwo\n",
        "* jeśli go nie ma liście, bierzemy prawdopodobieństwo według wzoru:\n",
        "\n",
        "\\begin{equation}\n",
        "P( word_N | word_{N-1}, word_{N-2}, ...., word_1 ) = \\\\\n",
        "P( word_N | word_{N-1}, word_{N-2}, ...., word_2 ) \\cdot \\text{backoff-weight}(  word_{N-1} | word_{N-2}, ...., word_1 )\n",
        "\\end{equation}\n",
        "\n",
        "* jeśli brakuje prawdopodobieństwa n-gramu mniejszego stopnia, wtedy rekurencyjnie stosujemy ten sam wzór aż do unigramów (które wszystkie powinny być zdefiniowane)\n",
        "* jeśli brakuje wagi back-off, zakładmy wartość 1 (czyli 0 w skali logarytmicznej)\n",
        "\n",
        "Na przykład, prawdopodobieństwo n-gramu \"*ala ma*\" jest następujące:\n",
        "\n",
        "\\begin{equation}\n",
        "P(ma|ala) = 10^{-0.1760913} = 0.6666666038148176\n",
        "\\end{equation}\n",
        "\n",
        "A prawdopodobieństwo n-gramu \"*jan ma psa*\":\n",
        "\n",
        "\n",
        "\\begin{equation}\n",
        "P(psa|jan,ma) = P(psa|ma)*bwt(ma|jan)=10^{(-0.69897+0)}=0.20000000199681048\n",
        "\\end{equation}\n",
        "\n",
        "Użyjmy prostej biblioteki `arpa` żeby potwierdzić powyższe obliczenia. Dokumentacja do biblioteki jest [tutaj](https://pypi.org/project/arpa/). Bibliotekę trzeba najpierw zainstalować:"
      ],
      "metadata": {
        "id": "RpCUk9sZNxpO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install arpa"
      ],
      "metadata": {
        "id": "R3-HO-sZN3Zy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Teaz zaimportuj całą bibliotekę `arpa` i użyj metody `loadf` żeby wczytać wyżej stworzony plik `out.arpa`. Metoda ta zwraca listę modeli, a nas oczywiście interesuje pierwszy (i jedyny) model.\n",
        "\n",
        "Po wczytaniu modelu można użyć wiele różnych metod. Nas interesuje metoda `p` która wylicza prawdopodobieństwo n-gramu. Dla powyższych przykładów, w pierwszym przypadku należy podać jako argument `'ala ma'` a w drugim `'jan ma psa'`:"
      ],
      "metadata": {
        "id": "uBtkWReD2gkQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "xqVGMboFNwUD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Jeśli wszystko poszło dobrze, wartości powinny być takie same jak wyżej. Dla zainteresowanych modelami ARPA polecam poczytać (niedługi i nieskomplikowany) kod źródłowy tej biblioteki.\n",
        "\n",
        "Jedną z podstatowych miar jakości modelu języka to tzw. *perplexity*. Liczymy go stotując wytrenowany model języka na niezależnym zbiorze testowym. Zróbmy przykładowy zbiór zawierający jedno zdanie `ala ma osę` i zapiszmy w pliku `eval.txt`:"
      ],
      "metadata": {
        "id": "BE4gxDESOBrW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Z41_PVSwOB9I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Do wyliczenia perplexity użyjemy programu `ngram` i użyjemy w nim opcję `-lm out.arpa` do wczytania pliku z modelem oraz `-ppl eval.txt` żeby policzyć perplexity na wybranym pliku:"
      ],
      "metadata": {
        "id": "Pk8JFNfHODdF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "dlzsjRXSOGNe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Wyniki zawierają ilość zdań, słów i wyrazów spoza słownika (OOV - out-of-vocabulary). Zawiera też wyliczony logprob całego korpusu oraz perplexity wyczlione uwzględniając i nieuwzlgędniając sztucznie dodane tokeny `<s>` oraz `</s>`. Im mniejsza wartość PPL, tym model lepiej opisuje testowy zbiór tekstów.\n",
        "\n",
        "Program `ngram` ma mnóstwo zastosowań, głównie związanych z edycją i manipulacją wytrenowanego modelu języka. Ma też opcję `-gen <N>` do wygenerowania losowych zdań z konkretnego modelu języka:"
      ],
      "metadata": {
        "id": "_Gxq0gD1OKB8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "DiVLgUmIOHxn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Praca domowa 3\n",
        "\n",
        "Zrób model języka z większego tekstu, np ściągniętego stąd:\n",
        "\n",
        "http://users.pja.edu.pl/~danijel/zum/sejm-text.xz\n",
        "\n",
        "Sprawdź perplexity na przykładowym tekście i wygeneruj jakiś losowy tekst."
      ],
      "metadata": {
        "id": "xB3mquMb3jnv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "lMPohLSsOfmO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}